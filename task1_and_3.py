# -*- coding: utf-8 -*-
"""Task1-folds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Anp1CA2ACBwLarkTbGSNSqBvX8_7oMD
"""

# !pip uninstall osqp
# !pip install scikit-survival
# !pip install scikit-learn
# !pip install lifelines

import sksurv as sv
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
import numpy as np
from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import train_test_split
from sksurv.nonparametric import kaplan_meier_estimator
from sksurv.datasets import \
    get_x_y  # Convenient for extracting X,y data from a dataframe, to be passed on to a fitter such as Cox PH.
# from lifelines import CoxPHFitter # Not used
from matplotlib import pyplot as plt
from sksurv.metrics import concordance_index_censored
from sklearn.exceptions import ConvergenceWarning
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import VarianceThreshold
import pickle5 as pickle
from sklearn.model_selection import RandomizedSearchCV
import sys


# ------------------------------------ Utility Functions ---------------------------------------------------------------
def transform_and_remove_low_var(df, var_threshold=1e-3):
    # assumes data_frames are as read from source (columns are observations, rows are features)
    df = df.transform(lambda x: np.log(1 + x) / np.log(2))
    return df[df.var(axis='columns') > var_threshold].T


def get_ci_score(model, X_test, y_test):
    prediction = model.predict(X_test)
    result = concordance_index_censored(y_test["dead_bool"], y_test["event_days_to"], prediction)
    return result[0]


def keep_top_k_variance_features(df, k):
    """Keeps top k most varianced features of df. NOT USED after we moved
    to more elaborate feature selection"""
    vars = df.var(axis='rows')
    sorted_vars = vars.sort_values(ascending=False)
    top_k_vars_series = sorted_vars[:k]
    top_k_features = [feature for feature in top_k_vars_series.index]
    res = df.drop([col for col in df.columns.tolist() if col not in top_k_features], axis=1)
    return res


def keep_top_k_mutual_features(df_list, k, add_to_end_lst=[]):
    """Takes a list of data frames, computes the top-k variance features that are mutual to all
    of them, and cleans each data frame to keep only those features. add_to_end_lst is used to insert special columns at
    the end of each data base, and drop those columns from anywhere before that.
    NOT USED after we moved to more elaborate feature selection"""
    feature_to_score = {}
    feature_appearances = {}
    for df in df_list:
        vars = df.var(axis='rows')
        sorted_vars = vars.sort_values(ascending=False)
        i = 0
        # return sorted_vars.index
        for feature in sorted_vars.index:
            # update score of feature according to its variance score in current df
            if feature not in feature_to_score:
                feature_to_score[feature] = 0
            feature_to_score[feature] += i

            # update number of times we encountered this. only features common to all df's can enter the final output
            if feature not in feature_appearances:
                feature_appearances[feature] = 0
            feature_appearances[feature] += 1
            i += 1

    num_tables = len(df_list)
    feature_to_score = {key: value for (key, value) in feature_to_score.items() if
                        feature_appearances[key] == num_tables}
    # return feature_appearances
    top_k_mutual_features = sorted(feature_to_score, key=lambda curr_feature: feature_to_score[curr_feature])[:k]

    for elem in add_to_end_lst:
        if elem in top_k_mutual_features:
            top_k_mutual_features.remove(elem)

    top_k_mutual_features += add_to_end_lst
    res = []
    for df in df_list:
        new_df = df.drop([col for col in df.columns.tolist() if col not in top_k_mutual_features], axis=1)
        res.append(new_df)
    return res


def get_instances_for_folds(fold_path, folds_to_use):
    """Returns all relevant instances for the provided list of folds"""
    folds = pd.read_csv(fold_path, sep=",", header=None)
    folds_list = []
    for i in range(5):
        if i in folds_to_use:
            folds_list.append(folds.iloc[i].to_list())
    res = []
    for fold in folds_list:
        for instance in fold:
            instance = str(instance).replace(".", "-")
            res.append(instance)
    return res


def keep_only_selected_features_by_count(selected_features_path, count):
    """returns  features received from input feature selection. returns only top
    'count'"""
    features_df = pd.read_csv(selected_features_path)
    count = min(count, len(features_df['feature_name']) - 1)
    return features_df.iloc[:count, 1].to_numpy()


def create_X_for_task_1_and_3_input(data_path, selected_features_path="", selected_features_count=0):
    """ Creates the input X matrix from the data for task 1 and 3 model predictors. if selected_features_path is
     provided, it reads the features from the file in that path and keeps only them in the matrix"""
    # Import omic data
    methy_df = pd.read_table(data_path + "/methy").drop_duplicates()
    exp_df = pd.read_table(data_path + "/exp").drop_duplicates()
    mirna_df = pd.read_table(data_path + "/mirna").drop_duplicates()

    # clean omic data
    methy_df = methy_df.T
    exp_df = transform_and_remove_low_var(exp_df, 0)
    mirna_df = transform_and_remove_low_var(mirna_df, 0)

    # Concatenate datasets using pandas pipe
    merged_X = (mirna_df.pipe(pd.merge, exp_df, left_index=True, right_index=True)
                .pipe(pd.merge, methy_df, left_index=True, right_index=True))

    # preform feature selection based on top-variance features or other input
    # feature selection.
    if selected_features_path:
        selected_features = keep_only_selected_features_by_count(selected_features_path, selected_features_count)
        merged_X = merged_X.drop([col for col in merged_X.columns.tolist() if col not in selected_features], axis=1)

    return merged_X

def create_X_and_y(data_path, k_most_variance_limit, fold_path="",
                   fold_numbers_to_include=[], selected_features_path="", selected_features_count=0):
    """ Creates the input X matrix and y vector from the input data for model training. if selected_features_path is
         provided, it reads the features in the path and keeps only them in the matrix. Similarly,
         if k_most_variance_limit is bigger than 0 then it only keeps top k variance features as feature selection"""
    # Import omic data
    methy_df = pd.read_table(data_path + "/methy").drop_duplicates()
    exp_df = pd.read_table(data_path + "/exp").drop_duplicates()
    mirna_df = pd.read_table(data_path + "/mirna").drop_duplicates()
    clinical_df = pd.read_table(str("{}/clinical".format(data_path)))[['last_contact_days_to', 'death_days_to']]

    # get fold instances to include in this data and only keep it
    if fold_path:
        instances_to_use = get_instances_for_folds(fold_path, fold_numbers_to_include)
        methy_df = methy_df.drop([col for col in methy_df.columns.tolist() if col not in instances_to_use], axis=1)
        exp_df = exp_df.drop([col for col in exp_df.columns.tolist() if col not in instances_to_use], axis=1)
        mirna_df = mirna_df.drop([col for col in mirna_df.columns.tolist() if col not in instances_to_use], axis=1)
        clinical_df = clinical_df.T.drop([col for col in clinical_df.T.columns.tolist() if col not in instances_to_use],
                                         axis=1).T

    # clean omic data
    methy_df = methy_df.T
    exp_df = transform_and_remove_low_var(exp_df, 0)
    mirna_df = transform_and_remove_low_var(mirna_df, 0)

    # Import patient data and combine the two duration columns into one.
    clinical_df['death_days_to'] = clinical_df['death_days_to'].apply(lambda x: x if x.isdigit() else 0).astype(int)
    clinical_df['last_contact_days_to'] = clinical_df['last_contact_days_to'].apply(
        lambda x: x if x.isdigit() else 0).astype(int)
    clinical_df['event_days_to'] = clinical_df['last_contact_days_to'] + clinical_df['death_days_to']

    clinical_df['censored'] = (clinical_df['death_days_to'] == 0).astype(int)
    clinical_df['dead'] = 1 - clinical_df['censored']
    clinical_df['dead_bool'] = clinical_df['dead'].astype(bool)

    clinical_df = clinical_df[['dead_bool', 'event_days_to']]

    # Concatenate datasets using pandas pipe
    merged_X = (mirna_df.pipe(pd.merge, exp_df, left_index=True, right_index=True)
                .pipe(pd.merge, methy_df, left_index=True, right_index=True))

    # preform feature selection based on top-variance features or other input
    # feature selection.
    if k_most_variance_limit > 0:
        merged_X = keep_top_k_variance_features(merged_X, k_most_variance_limit)

    if selected_features_path:
        selected_features = keep_only_selected_features_by_count(selected_features_path, selected_features_count)
        merged_X = merged_X.drop([col for col in merged_X.columns.tolist() if col not in selected_features], axis=1)

    merged_df = (clinical_df.pipe(pd.merge, merged_X, left_index=True, right_index=True))

    X, y = get_x_y(merged_df, attr_labels=['dead_bool', 'event_days_to'], pos_label=True)

    return X, y


def visualize_alpha_values(best_model, best_coefs, alphas):
    best_l1_ratio = best_model.l1_ratio
    non_zero = np.sum(best_coefs.iloc[:, 0] != 0)
    print("Number of non-zero coefficients: {}".format(non_zero))
    print("Best l1_ratio: {}".format(best_l1_ratio))

    non_zero_coefs = best_coefs.query("coefficient != 0")
    coef_order = non_zero_coefs.abs().sort_values("coefficient").index

    _, ax = plt.subplots(figsize=(6, 8))
    non_zero_coefs.loc[coef_order].plot.barh(ax=ax, legend=False)
    ax.set_xlabel("coefficient")
    ax.grid(True)


def visualize_ci_for_alpha_values(model_results_table, pure_results, alphas):
    mean = model_results_table.mean_test_score
    std = model_results_table.std_test_score

    fig, ax = plt.subplots(figsize=(9, 6))
    ax.plot(alphas, mean)
    ax.fill_between(alphas, mean - std, mean + std, alpha=.15)
    ax.set_xscale("log")
    ax.set_ylabel("concordance index")
    ax.set_xlabel("alpha")
    ax.axvline(pure_results.best_params_["alphas"][0], c="C1")
    ax.axhline(0.5, color="grey", linestyle="--")
    ax.grid(True)

# ----------------------------------Model Training Funcitons------------------------------------------------------------

"""Based on: https://scikit-survival.readthedocs.io/en/latest/user_guide/coxnet.html

We can use this to find the implement cross validation (under "choosing penalty strength")
"""


def get_grid_search_results(parameters, X_train, y_train, max_iter):
    """Preforms grid based CV across the different hyper-parameters to select the best prediction model, then
    returns it"""

    gcv = GridSearchCV(
        CoxnetSurvivalAnalysis(max_iter=max_iter),
        param_grid=parameters,
        error_score=0.5,
        n_jobs=4).fit(X_train, y_train)

    return gcv


def get_random_search_results(parameters, X_train, y_train, max_iter):
    """Preforms Random based CV across the different hyper-parameters to select the best prediction model, then
     returns it"""
    cvf = RandomizedSearchCV(
        CoxnetSurvivalAnalysis(max_iter=max_iter),
        param_distributions=parameters,
        error_score=0.5,
        n_jobs=4).fit(X_train, y_train)

    return cvf

def train_coxnet_model_for_baseline(cancer_data_path, folds_data_path,
                                    test_fold_index, max_iter, top_k_variance_limit,
                                    selected_features_path, selected_features_count, present_output=True,
                                    model_output_path=""):
    """Trains an elastic CoxNetPH model for the data in the input files. Selects hyper parameters based
    on grid search CV. Preforms feature selection based on input variables (pre-defined selection or top
    k variance). Saves the trained model if there is a specified path for it."""

   # create train and test sets
    X_train, y_train = create_X_and_y(cancer_data_path, top_k_variance_limit, folds_data_path,
                                      [i for i in range(5) if i != test_fold_index], selected_features_path,
                                      selected_features_count)
    X_test, y_test = create_X_and_y(cancer_data_path, top_k_variance_limit, folds_data_path, [test_fold_index],
                                    selected_features_path, selected_features_count)

    # train a simple coxnet model to get a range of possible alphas
    cox_net = CoxnetSurvivalAnalysis(max_iter=max_iter)
    cox_net.fit(X_train, y_train)
    initial_alphas = cox_net.alphas_

    params = {"alphas": [[v] for i, v in enumerate(initial_alphas) if i % 5 == 0] + [[v * 10] for i, v in
                                                                                     enumerate(initial_alphas) if
                                                                                     i % 5 == 0]
        , "l1_ratio": [0.00101, 0.1, 0.5, 0.7, 1]}

    gcv_results = get_grid_search_results(params, X_train, y_train, max_iter)
    gcv_results_table = pd.DataFrame(gcv_results.cv_results_)
    best_model = gcv_results.best_estimator_
    alphas = gcv_results_table.param_alphas.map(lambda x: x[0])

    best_l1_ratio = best_model.l1_ratio
    best_coefs = pd.DataFrame(
        best_model.coef_,
        index=X_train.columns,
        columns=["coefficient"]
    )

    best_model_cv = get_ci_score(best_model, X_test, y_test)

    if present_output:
        # present results
        print("Best model C.I for test set on fold #{} is: {}".format(test_fold_index, best_model_cv))
        print("Best alpha: {}".format(best_model.alphas_))
        print("Best l1 ratio: {}".format(best_l1_ratio))
        visualize_alpha_values(best_model, best_coefs, alphas)
        visualize_ci_for_alpha_values(gcv_results_table, gcv_results, alphas)
        print("Internal CV scores while training: ")
        print(cross_val_score(best_model, X_train, y_train,
                              cv=5))  # check what to actually cross validate on - holdout or all?

    # save model
    if model_output_path:
        pickle.dump(best_model, open(model_output_path, "wb"))
    return best_model, best_model_cv


def train_coxnet_model_for_baseline_using_random_search_cv(cancer_data_path,
                                                           folds_data_path,
                                                           test_fold_index, max_iter, top_k_variance_limit,
                                                           selected_features_path, selected_features_count,
                                                           present_output=True, model_output_path=""):
    """Trains an elastic CoxNetPH model for the data in the input files. Selects hyper parameters based
    on random search CV. Preforms feature selection based on input variables (pre-defined selection or top
    k variance). Saves the trained model if there is a specified path for it."""
    # create train and test sets, and clean the data
    X_train, y_train = create_X_and_y(cancer_data_path, top_k_variance_limit, folds_data_path,
                                      [i for i in range(5) if i != test_fold_index], selected_features_path,
                                      selected_features_count)
    X_test, y_test = create_X_and_y(cancer_data_path, top_k_variance_limit, folds_data_path, [test_fold_index],
                                    selected_features_path, selected_features_count)

    # train a simple coxnet model to get a range of possible alphas
    cox_net = CoxnetSurvivalAnalysis(max_iter=max_iter)
    cox_net.fit(X_train, y_train)
    initial_alphas = cox_net.alphas_

    # create hyper-parameters for CV and train model
    params = {"alphas": [[v] for i, v in enumerate(initial_alphas) if i % 5 == 0] + [[v * 10] for i, v in
                                                                                     enumerate(initial_alphas) if
                                                                                     i % 5 == 0]
        , "l1_ratio": [0.0001, 0.1, 0.3, 0.5, 0.9, 1]}

    gcv_results = get_random_search_results(params, X_train, y_train, max_iter)
    gcv_results_table = pd.DataFrame(gcv_results.cv_results_)
    best_model = gcv_results.best_estimator_
    alphas = gcv_results_table.param_alphas.map(lambda x: x[0])

    best_l1_ratio = best_model.l1_ratio
    best_coefs = pd.DataFrame(
        best_model.coef_,
        index=X_train.columns,
        columns=["coefficient"]
    )

    best_model_cv = get_ci_score(best_model, X_test, y_test)
    if present_output:
        # present results
        print("Best model C.I for test set on fold #{} is: {}".format(test_fold_index, best_model_cv))
        print("Best alpha: {}".format(best_model.alphas_))
        print("best l1 ratio: {}".format(best_l1_ratio))
        visualize_alpha_values(best_model, best_coefs, alphas)
        visualize_ci_for_alpha_values(gcv_results_table, gcv_results, alphas)
        print("Internal CV scores while training: ")
        print(cross_val_score(best_model, X_train, y_train,
                              cv=5))

    # save model
    if model_output_path:
        pickle.dump(best_model, open(model_output_path, "wb"))

    return best_model, best_model_cv


def get_cv_scores_for_5_folds_random_search(cancer_data_path, folds_data_path,
                                            max_iter, top_k_variance_limit, selected_features_path,
                                            selected_features_count, present_output):
    """for each fold, creates the proper test/train data split, trains the model and predicts using
    grid search CV"""
    res = []
    for i in range(5):
        print("Fold number: ", i)
        estimator_model_i, cv_i = train_coxnet_model_for_baseline_using_random_search_cv(cancer_data_path,
                                                                                         folds_data_path, i,
                                                                                         max_iter,
                                                                                         top_k_variance_limit,
                                                                                         selected_features_path,
                                                                                         selected_features_count,
                                                                                         present_output)
        res.append(cv_i)
        print("-----------------------------------------------")

    print("C.I scores for each respective fold: ")
    print(res)
    print("Average C.I: ", str(sum(res) / 5))
    return res


def get_cv_scores_for_5_folds_search(cancer_data_path, folds_data_path,
                                     max_iter, top_k_variance_limit, selected_features_path,
                                     selected_features_count, present_output):
    """for each fold, creates the proper test/train data split, trains the model and predicts using
    grid search CV"""
    res = []
    for i in range(5):
        print("Fold number: ", i)
        estimator_model_i, cv_i = train_coxnet_model_for_baseline(cancer_data_path, folds_data_path, i,
                                                                  max_iter, top_k_variance_limit,
                                                                  selected_features_path, selected_features_count,
                                                                  present_output)
        res.append(cv_i)
        print("-----------------------------------------------")

    print("C.I scores for each respective fold: ")
    print(res)
    print("Average C.I: ", str(sum(res) / 5))
    return res


def create_prediction(cancer_type, data_path, output_data_path):
    """Loads the relevant model (which is already trained) and predicts survival ranking for input data.
    The Scikit Survival model predict() function returns values of the linear predictor,
     meaning the higher the score - the higher the chances of this instance to have an even soon (and thus
     it will rank higher)"""
    cancer_type = cancer_type.upper()
    model = pickle.load(open("data/" + cancer_type + "/model_" + cancer_type, "rb"))
    features_num = 30 if cancer_type == "LGG" else 600
    X = create_X_for_task_1_and_3_input(data_path, "data/" + cancer_type + "/features.csv", features_num)
    pred = model.predict(X)
    pred_df = pd.DataFrame(data=pred, index=X.index, columns=["result"])
    sorted_pred_df = pred_df.sort_values(by=["result"], ascending=False)
    sorted_index_by_risk = list(sorted_pred_df.index)
    result_df = pd.DataFrame(index=X.index,
                             data=np.array([sorted_index_by_risk.index(sample) + 1 for sample in X.index]),
                             columns=["order"])
    result_df.to_csv(output_data_path, sep="\t")



# ----------------------- Actual usage example - Function Calls to Train +_Test Models on provided Folds----------------

# Random Search CV trained Models (grid search CV is similar, only with call to get_cv_scores_for_5_folds()

# print("________BLCA_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/BLCA", "data/BLCA/BLCA_folds",
#                                         max_iter=100000, top_k_variance_limit=0, selected_features_path="data/BLCA/features.csv", selected_features_count=400, present_output=True)
# print("--------------------------------------------------------------------------------------------------------------------------------------------")
# print("________BRCA_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/BRCA", "data/BRCA/BRCA_folds",
#                                         max_iter=100000, top_k_variance_limit=0, selected_features_path="data/BRCA/features.csv", selected_features_count=400, present_output=True)
# print("--------------------------------------------------------------------------------------------------------------------------------------------")
#
# print("________HNSC_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/HNSC", "data/HNSC/HNSC_folds",
#                                         max_iter=80000, top_k_variance_limit=0, selected_features_path="data/HNSC/features.csv", selected_features_count=400, present_output=True)
# print("--------------------------------------------------------------------------------------------------------------------------------------------")
#
# print("________LAML_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/LAML", "data/LAML/LAML_folds",
#                                         max_iter=100000, top_k_variance_limit=0, selected_features_path="data/LAML/features.csv", selected_features_count=400, present_output=True)
# print("--------------------------------------------------------------------------------------------------------------------------------------------")
#
# print("________LGG_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/LGG", "data/LGG/LGG_folds",
#                                         max_iter=100000, top_k_variance_limit=0, selected_features_path="data/LGG/features.csv", selected_features_count=30, present_output=True)
# print("--------------------------------------------------------------------------------------------------------------------------------------------")
#
# print("________LUAD_RANDOM_MODEL_RESULTS________________:")
# get_cv_scores_for_5_folds_random_search("data/LUAD", "data/LUAD/LUAD_folds",
#                                         max_iter=100000, top_k_variance_limit=0, selected_features_path="data/LUAD/features.csv", selected_features_count=30, present_output=True)


# ----------------------- Actual usage example - Function calls to train models for future predictions------------------
# print("________BLCA_RANDOM_FINAL_MODEL______________:")
# train_coxnet_model_for_baseline_using_random_search_cv("data/BLCA", "", 0,
                                        # max_iter=100000, top_k_variance_limit=0, selected_features_path="data/BLCA/features.csv", selected_features_count=400, present_output=True, model_output_path="data/BLCA/model")
